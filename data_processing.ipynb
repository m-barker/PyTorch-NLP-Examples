{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of token constants\n",
    "PAD_TKN = 0 # for padding sequence lengths\n",
    "CLS_TKN = 101 # for the start of the sequence\n",
    "SEP_TKN = 102 # for the end of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "result = tokenizer.encode(\"Hello world!\", add_special_tokens=True, return_tensors=\"pt\")\n",
    "decoded_result = tokenizer.decode(result[0])\n",
    "print(f\"Encoded sentence: {result}\")\n",
    "print(f\"Decoded sentence: {decoded_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([101, 7592, 2088, 999, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a much more limited vocabularly, we can simply define a dictionary that creates a token per word and maps from words to token IDs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_TOKEN_MAPPING = {\n",
    "    \"<PAD>\" : 0,\n",
    "    \"<BOS>\" : 1,\n",
    "    \"<EOS>\" : 2,\n",
    "    \"the\" : 3,\n",
    "    \"quick\" : 4,\n",
    "    \"brown\" : 5,\n",
    "    \"fox\" : 6,\n",
    "    \"jumps\" : 7,\n",
    "    \"over\" : 8,\n",
    "    \"lazy\" : 9,\n",
    "    \"dog\" : 10,\n",
    "\n",
    "}\n",
    "\n",
    "FRENCH_TOKEN_MAPPING = {\n",
    "    \"<PAD>\" : 0,\n",
    "    \"<BOS>\" : 1,\n",
    "    \"<EOS>\" : 2,\n",
    "    \"le\" : 3,\n",
    "    \"renard\" : 4,\n",
    "    \"brun\" : 5,\n",
    "    \"rapide\" : 6,\n",
    "    \"saute\" : 7,\n",
    "    \"par-dessus\" : 8,\n",
    "    \"chien\" : 9,\n",
    "    \"paresseux\" : 10\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From token IDs to token embeddings\n",
    "Adapted from this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Class for converting token IDs of a fixed vocabulary size into vectors of a fixed length\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, padding_idx: int = 0):\n",
    "        # The padding index prevents gradients from being propagated to the \n",
    "        # embedding parameters for padding tokens\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
    "        self._embed_dim = emb_dim\n",
    "        self._vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ENGLISH_TOKEN_MAPPING)\n",
    "embedding_dim = 128\n",
    "token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "tokens = torch.tensor([ENGLISH_TOKEN_MAPPING[\"the\"]], dtype=torch.long)\n",
    "embedding = token_embedding(tokens)\n",
    "print(f\"Embedding for 'the':\\n{embedding}\\nShape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "We want to add information to the word embedding to encoded information about their positions in a sequence.\n",
    "\n",
    "Below is taken from the official [PyTorch transformers tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            \n",
    "        Adds positional encoding to a given sequential input tensor.\n",
    "            \n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoder = PositionalEncoding(embedding_dim)\n",
    "test_sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "test_tokens = [ENGLISH_TOKEN_MAPPING[token] for token in test_sentence.split()]\n",
    "test_tokens = torch.tensor(test_tokens, dtype=torch.long)\n",
    "test_embedding = token_embedding(test_tokens)\n",
    "test_embedding = test_embedding.unsqueeze(1) # add batch dim\n",
    "positional_encoder(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the input for the translation task\n",
    "As discussed in this [blog post](https://kikaben.com/transformers-encoder-decoder/), we typically use teacher forcing to prepare the sequence of inputs in parallel.\n",
    "\n",
    "For example, the first input to the decoder block would be the encoded `the quick brown fox jumps over the lazy dog` along with the `<BOS>` tag. The decoder would then hopefully output the first word of the translation `le`. The next sets of inputs to the decoder would be the same apart from `<BOS> le` etc.\n",
    "\n",
    "The teaching forcing means we give the ground-truth tokens at each step of the translations, rather than auto-regressively waiting for the decoder to generate each required token, which means that we can parallelise the input. \n",
    "\n",
    "This parallelisation is done using **masked attention** which sets the attention value to 0 for any token that the decoder is not \"allowed\" to see yet.\n",
    "\n",
    "Our full target sequence is `<BOS> le renard brun rapide saute par-dessus le chien paresseux <EOS>`\n",
    "\n",
    "The first attention mask is thus `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]` as the decoder is only allowed to see the `<BOS>` when predicting the first output token.\n",
    "\n",
    "The second attention mask is `[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]` etc.\n",
    "\n",
    "We can group these together in a matrix:\n",
    "\n",
    "```\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "```\n",
    "\n",
    "This can be quickly done using an existing PyTorch function (which uses zeros un-masked values and -infs for masked values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Transformer.generate_square_subsequent_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 n_head: int, \n",
    "                 num_encoder_layers: int, \n",
    "                 num_decoder_layers: int, \n",
    "                 dim_feedforward: int = 512, \n",
    "                 dropout: float = 0.1, \n",
    "                 activation: str = \"relu\", \n",
    "                 source_vocab_size: int = 100,\n",
    "                 target_vocab_size: int = 100, \n",
    "                 max_seq_length: int = 100, \n",
    "                 device: torch.device = torch) -> None:\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=d_model, \n",
    "                                          nhead=n_head, \n",
    "                                          num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers, \n",
    "                                          dim_feedforward=dim_feedforward, \n",
    "                                          dropout=dropout, \n",
    "                                          activation=activation)\n",
    "        self._target_vocab_size = target_vocab_size\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._device = device\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        self.pe = PositionalEncoding(d_model, dropout)\n",
    "        self.src_embedding = TokenEmbedding(source_vocab_size, d_model)\n",
    "        self.tgt_embedding = TokenEmbedding(target_vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                tgt: torch.Tensor) -> torch.Tensor:\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        src = self.pe(src)\n",
    "        tgt = self.pe(tgt)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(self._device)\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.final_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "target_sentence = \"<BOS> le renard brun rapide saute par-dessus le chien paresseux <EOS>\"\n",
    "source_tokens = [ENGLISH_TOKEN_MAPPING[token] for token in source_sentence.split()]\n",
    "target_tokens = [FRENCH_TOKEN_MAPPING[token] for token in target_sentence.split()]\n",
    "print(f\"Source tokens: {source_tokens}\")\n",
    "print(f\"Target tokens: {target_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerEncoderDecoder(d_model=512,\n",
    "                                        n_head=2,\n",
    "                                        num_encoder_layers=6,\n",
    "                                        num_decoder_layers=6,\n",
    "                                        source_vocab_size=11,\n",
    "                                        target_vocab_size=11,\n",
    "                                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "source_tokens_tensor = torch.tensor(source_tokens, dtype=torch.long).unsqueeze(1)\n",
    "target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long).unsqueeze(1)\n",
    "print(f\"Source tokens shape: {source_tokens_tensor.shape}\")\n",
    "print(f\"Target tokens shape: {target_tokens_tensor.shape}\")\n",
    "\n",
    "result = transformer(source_tokens_tensor, target_tokens_tensor).squeeze(1) # remove batch dim\n",
    "print(f\"Output shape: {result.shape}\")\n",
    "\n",
    "# Run through softmax to get probabilities\n",
    "result = nn.functional.softmax(result, dim=-1)\n",
    "predicted_token_ids = torch.argmax(result, dim=-1)\n",
    "print(f\"Predicted token IDs: {predicted_token_ids}\")\n",
    "predicted_words = [list(FRENCH_TOKEN_MAPPING.keys())[list(FRENCH_TOKEN_MAPPING.values()).index(token_id)] for token_id in predicted_token_ids]\n",
    "print(f\"Predicted Translation: {' '.join(predicted_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "transformer = TransformerEncoderDecoder(d_model=512,\n",
    "                                        n_head=2,\n",
    "                                        num_encoder_layers=6,\n",
    "                                        num_decoder_layers=6,\n",
    "                                        source_vocab_size=11,\n",
    "                                        target_vocab_size=11,\n",
    "                                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "source_tokens_tensor = torch.tensor(source_tokens, dtype=torch.long).unsqueeze(1)\n",
    "target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long).unsqueeze(1)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "print_every = 100\n",
    "for train_iter in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    result = transformer(source_tokens_tensor, target_tokens_tensor).squeeze(1)\n",
    "    result = nn.functional.softmax(result, dim=-1)\n",
    "    loss = loss_function(result.view(-1, transformer._target_vocab_size), target_tokens_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if train_iter % print_every == 0:\n",
    "        print(f\"Iteration {train_iter}, Loss: {loss.item()}\")\n",
    "        predicted_token_ids = torch.argmax(result, dim=-1)\n",
    "        predicted_words = [list(FRENCH_TOKEN_MAPPING.keys())[list(FRENCH_TOKEN_MAPPING.values()).index(token_id)] for token_id in predicted_token_ids]\n",
    "        print(f\"Predicted Translation: {' '.join(predicted_words)}\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
