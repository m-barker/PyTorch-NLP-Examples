{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattbarker/dev/venvs/deep_learning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of token constants\n",
    "PAD_TKN = 0 # for padding sequence lengths\n",
    "CLS_TKN = 101 # for the start of the sequence\n",
    "SEP_TKN = 102 # for the end of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sentence: tensor([[ 101, 7592, 2088,  999,  102]])\n",
      "Decoded sentence: [CLS] hello world! [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "result = tokenizer.encode(\"Hello world!\", add_special_tokens=True, return_tensors=\"pt\")\n",
    "decoded_result = tokenizer.decode(result[0])\n",
    "print(f\"Encoded sentence: {result}\")\n",
    "print(f\"Decoded sentence: {decoded_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 7592, 2088, 999, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a much more limited vocabularly, we can simply define a dictionary that creates a token per word and maps from words to token IDs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_TOKEN_MAPPING = {\n",
    "    \"<PAD>\" : 0,\n",
    "    \"<BOS>\" : 1,\n",
    "    \"<EOS>\" : 2,\n",
    "    \"the\" : 3,\n",
    "    \"quick\" : 4,\n",
    "    \"brown\" : 5,\n",
    "    \"fox\" : 6,\n",
    "    \"jumps\" : 7,\n",
    "    \"over\" : 8,\n",
    "    \"lazy\" : 9,\n",
    "    \"dog\" : 10,\n",
    "\n",
    "}\n",
    "\n",
    "FRENCH_TOKEN_MAPPING = {\n",
    "    \"<PAD>\" : 0,\n",
    "    \"<BOS>\" : 1,\n",
    "    \"<EOS>\" : 2,\n",
    "    \"le\" : 3,\n",
    "    \"renard\" : 4,\n",
    "    \"brun\" : 5,\n",
    "    \"rapide\" : 6,\n",
    "    \"saute\" : 7,\n",
    "    \"par-dessus\" : 8,\n",
    "    \"chien\" : 9,\n",
    "    \"paresseux\" : 10\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From token IDs to token embeddings\n",
    "Adapted from this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Class for converting token IDs of a fixed vocabulary size into vectors of a fixed length\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, padding_idx: int = 0):\n",
    "        # The padding index prevents gradients from being propagated to the \n",
    "        # embedding parameters for padding tokens\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
    "        self._embed_dim = emb_dim\n",
    "        self._vocab_size = vocab_size\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'the':\n",
      "tensor([[ 0.8037, -0.0358, -1.1829, -0.3455, -2.1089,  0.2363,  1.1595, -0.6717,\n",
      "         -1.0007, -0.4557,  0.2388,  0.4289,  0.6512,  1.5092, -0.6770,  0.6057,\n",
      "          1.1825, -0.0660,  0.4837, -1.7217, -0.7760, -0.2422,  0.4692,  0.6317,\n",
      "         -2.8646,  2.1812,  0.2099, -0.3220,  1.7585, -1.0891,  2.0922, -2.9054,\n",
      "         -0.5375,  0.5031,  0.1555,  1.7352,  1.4221, -0.7985, -0.3126, -0.5879,\n",
      "         -0.1025, -0.4161,  0.3701, -0.2006,  1.4460, -0.8493, -2.2056,  0.1394,\n",
      "          1.7231,  0.7155, -0.2800,  0.9610, -0.1751, -1.0879, -0.8913, -0.8063,\n",
      "          0.4753, -0.0712, -1.3732,  1.1815, -1.4304,  1.2193, -0.9271, -0.1687,\n",
      "         -1.9771, -0.5979,  1.1737, -0.3849,  0.4964,  1.2381, -0.6243, -0.7886,\n",
      "         -0.2245, -0.0921, -0.8983,  0.1947,  1.7604, -2.6235,  0.5024, -0.0684,\n",
      "          0.2427,  0.2252, -2.0481,  0.6361,  0.8009,  0.3931, -0.5136,  0.2945,\n",
      "         -0.1167,  2.8122,  1.1558,  1.9226, -1.1016, -1.7632,  0.5140,  0.4155,\n",
      "          0.7965, -0.8381, -1.7288, -0.4766, -0.4793, -1.1310,  0.5840, -1.0116,\n",
      "          0.6179, -2.3534, -0.7929, -0.6745, -0.1086, -0.4610, -1.1056, -0.5585,\n",
      "         -0.1349, -0.3957,  0.5312,  0.9927,  1.1705, -0.6343,  1.4484,  0.7269,\n",
      "         -0.5910,  0.1873, -0.2156, -0.5138, -0.1848,  0.1708,  0.4305, -0.7256]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(ENGLISH_TOKEN_MAPPING)\n",
    "embedding_dim = 128\n",
    "token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
    "tokens = torch.tensor([ENGLISH_TOKEN_MAPPING[\"the\"]], dtype=torch.long)\n",
    "embedding = token_embedding(tokens)\n",
    "print(f\"Embedding for 'the':\\n{embedding}\\nShape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "We want to add information to the word embedding to encoded information about their positions in a sequence.\n",
    "\n",
    "Below is taken from the official [PyTorch transformers tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            \n",
    "        Adds positional encoding to a given sequential input tensor.\n",
    "            \n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8931,  1.0713, -1.3144,  ...,  1.3008,  0.4784,  0.3049]],\n",
       "\n",
       "        [[ 3.2182,  1.3569,  2.6505,  ..., -0.1593,  0.3387,  1.4701]],\n",
       "\n",
       "        [[ 2.5950,  0.1774,  0.1516,  ...,  1.1909, -0.0000,  0.7388]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5826,  1.0271, -2.2982,  ...,  1.3008,  0.0000,  0.3049]],\n",
       "\n",
       "        [[-0.7360,  0.4591, -0.1274,  ...,  2.8325,  0.1148,  0.1064]],\n",
       "\n",
       "        [[ 2.0990, -3.6456,  0.7737,  ...,  1.7929, -0.4480,  2.4312]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoder = PositionalEncoding(embedding_dim)\n",
    "test_sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "test_tokens = [ENGLISH_TOKEN_MAPPING[token] for token in test_sentence.split()]\n",
    "test_tokens = torch.tensor(test_tokens, dtype=torch.long)\n",
    "test_embedding = token_embedding(test_tokens)\n",
    "test_embedding = test_embedding.unsqueeze(1) # add batch dim\n",
    "positional_encoder(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the input for the translation task\n",
    "As discussed in this [blog post](https://kikaben.com/transformers-encoder-decoder/), we typically use teacher forcing to prepare the sequence of inputs in parallel.\n",
    "\n",
    "For example, the first input to the decoder block would be the encoded `the quick brown fox jumps over the lazy dog` along with the `<BOS>` tag. The decoder would then hopefully output the first word of the translation `le`. The next sets of inputs to the decoder would be the same apart from `<BOS> le` etc.\n",
    "\n",
    "The teaching forcing means we give the ground-truth tokens at each step of the translations, rather than auto-regressively waiting for the decoder to generate each required token, which means that we can parallelise the input. \n",
    "\n",
    "This parallelisation is done using **masked attention** which sets the attention value to 0 for any token that the decoder is not \"allowed\" to see yet.\n",
    "\n",
    "Our full target sequence is `<BOS> le renard brun rapide saute par-dessus le chien paresseux <EOS>`\n",
    "\n",
    "The first attention mask is thus `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]` as the decoder is only allowed to see the `<BOS>` when predicting the first output token.\n",
    "\n",
    "The second attention mask is `[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]` etc.\n",
    "\n",
    "We can group these together in a matrix:\n",
    "\n",
    "```\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "```\n",
    "\n",
    "This can be quickly done using an existing PyTorch function (which uses zeros un-masked values and -infs for masked values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Transformer.generate_square_subsequent_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 n_head: int, \n",
    "                 num_encoder_layers: int, \n",
    "                 num_decoder_layers: int, \n",
    "                 dim_feedforward: int = 512, \n",
    "                 dropout: float = 0.1, \n",
    "                 activation: str = \"relu\", \n",
    "                 source_vocab_size: int = 100,\n",
    "                 target_vocab_size: int = 100, \n",
    "                 max_seq_length: int = 100, \n",
    "                 device: torch.device = torch) -> None:\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=d_model, \n",
    "                                          nhead=n_head, \n",
    "                                          num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers, \n",
    "                                          dim_feedforward=dim_feedforward, \n",
    "                                          dropout=dropout, \n",
    "                                          activation=activation)\n",
    "        self._target_vocab_size = target_vocab_size\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._device = device\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        self.pe = PositionalEncoding(d_model, dropout)\n",
    "        self.src_embedding = TokenEmbedding(source_vocab_size, d_model)\n",
    "        self.tgt_embedding = TokenEmbedding(target_vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                tgt: torch.Tensor,\n",
    "                generate_mask: bool = True) -> torch.Tensor:\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        src = self.pe(src)\n",
    "        tgt = self.pe(tgt)\n",
    "        tgt_mask = None\n",
    "        if generate_mask:\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(self._device)\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.final_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tokens: [3, 4, 5, 6, 7, 8, 3, 9, 10]\n",
      "Target tokens: [1, 3, 4, 5, 6, 7, 8, 3, 9, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "source_sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "target_sentence = \"<BOS> le renard brun rapide saute par-dessus le chien paresseux <EOS>\"\n",
    "source_tokens = [ENGLISH_TOKEN_MAPPING[token] for token in source_sentence.split()]\n",
    "target_tokens = [FRENCH_TOKEN_MAPPING[token] for token in target_sentence.split()]\n",
    "print(f\"Source tokens: {source_tokens}\")\n",
    "print(f\"Target tokens: {target_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattbarker/dev/venvs/deep_learning/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tokens shape: torch.Size([9, 1])\n",
      "Target tokens shape: torch.Size([11, 1])\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Output shape: torch.Size([11, 11])\n",
      "Predicted token IDs: tensor([3, 3, 3, 3, 5, 5, 5, 3, 3, 3, 3])\n",
      "Predicted Translation: le le le le brun brun brun le le le le\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerEncoderDecoder(d_model=512,\n",
    "                                        n_head=2,\n",
    "                                        num_encoder_layers=6,\n",
    "                                        num_decoder_layers=6,\n",
    "                                        source_vocab_size=11,\n",
    "                                        target_vocab_size=11,\n",
    "                                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "source_tokens_tensor = torch.tensor(source_tokens, dtype=torch.long).unsqueeze(1)\n",
    "target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long).unsqueeze(1)\n",
    "print(f\"Source tokens shape: {source_tokens_tensor.shape}\")\n",
    "print(f\"Target tokens shape: {target_tokens_tensor.shape}\")\n",
    "\n",
    "result = transformer(source_tokens_tensor, target_tokens_tensor).squeeze(1) # remove batch dim\n",
    "print(f\"Output shape: {result.shape}\")\n",
    "\n",
    "# Run through softmax to get probabilities\n",
    "result = nn.functional.softmax(result, dim=-1)\n",
    "predicted_token_ids = torch.argmax(result, dim=-1)\n",
    "print(f\"Predicted token IDs: {predicted_token_ids}\")\n",
    "predicted_words = [list(FRENCH_TOKEN_MAPPING.keys())[list(FRENCH_TOKEN_MAPPING.values()).index(token_id)] for token_id in predicted_token_ids]\n",
    "print(f\"Predicted Translation: {' '.join(predicted_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 2.6233906745910645\n",
      "Predicted Translation: par-dessus <BOS> <BOS> <BOS> <BOS> <BOS> <EOS> <BOS> <BOS> <BOS>\n",
      "Iteration 10, Loss: 2.194302558898926\n",
      "Predicted Translation: rapide le saute le le le le le par-dessus brun\n",
      "Iteration 20, Loss: 1.9355199337005615\n",
      "Predicted Translation: renard le brun renard renard le le le <EOS> <EOS>\n",
      "Iteration 30, Loss: 0.9529194831848145\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien par-dessus <EOS>\n",
      "Iteration 40, Loss: 0.5133823156356812\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n",
      "Iteration 50, Loss: 0.15060295164585114\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n",
      "Iteration 60, Loss: 0.05734274536371231\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n",
      "Iteration 70, Loss: 0.022655215114355087\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n",
      "Iteration 80, Loss: 0.016434239223599434\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n",
      "Iteration 90, Loss: 0.014442960731685162\n",
      "Predicted Translation: le renard brun rapide saute par-dessus le chien paresseux <EOS>\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "transformer = TransformerEncoderDecoder(d_model=512,\n",
    "                                        n_head=2,\n",
    "                                        num_encoder_layers=6,\n",
    "                                        num_decoder_layers=6,\n",
    "                                        source_vocab_size=11,\n",
    "                                        target_vocab_size=11,\n",
    "                                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "source_tokens_tensor = torch.tensor(source_tokens, dtype=torch.long).unsqueeze(1)\n",
    "target_tokens_tensor = torch.tensor(target_tokens, dtype=torch.long).unsqueeze(1)\n",
    "input_target_tokens = target_tokens_tensor[:-1]\n",
    "target_target_tokens = target_tokens_tensor[1:]\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "print_every = 10\n",
    "for train_iter in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    result = transformer(source_tokens_tensor, input_target_tokens).squeeze(1)\n",
    "    loss = loss_function(result.view(-1, transformer._target_vocab_size), target_target_tokens.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if train_iter % print_every == 0:\n",
    "        print(f\"Iteration {train_iter}, Loss: {loss.item()}\")\n",
    "        result = nn.functional.softmax(result, dim=-1)\n",
    "        predicted_token_ids = torch.argmax(result, dim=-1)\n",
    "        predicted_words = [list(FRENCH_TOKEN_MAPPING.keys())[list(FRENCH_TOKEN_MAPPING.values()).index(token_id)] for token_id in predicted_token_ids]\n",
    "        print(f\"Predicted Translation: {' '.join(predicted_words)}\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Outside of training time, we will not have the target sequence for translation. Thus, we need a method to autoregressively generate the predicted translation, appending the generated token(s) to the input prompt as we go\n",
    "\n",
    "We do not need to apply any attention masking, as we can have our model pay attention to everything!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: <BOS> le\n",
      "Predicted Translation: <BOS> le renard\n",
      "Predicted Translation: <BOS> le renard brun\n",
      "Predicted Translation: <BOS> le renard brun rapide\n",
      "Predicted Translation: <BOS> le renard brun rapide saute\n",
      "Predicted Translation: <BOS> le renard brun rapide saute par-dessus\n",
      "Predicted Translation: <BOS> le renard brun rapide saute par-dessus le\n",
      "Predicted Translation: <BOS> le renard brun rapide saute par-dessus le chien\n",
      "Predicted Translation: <BOS> le renard brun rapide saute par-dessus le chien paresseux\n",
      "Predicted Translation: <BOS> le renard brun rapide saute par-dessus le chien paresseux <EOS>\n"
     ]
    }
   ],
   "source": [
    "trained_model = transformer\n",
    "input_sequence = source_tokens_tensor\n",
    "translated_input = torch.tensor([[FRENCH_TOKEN_MAPPING[\"<BOS>\"]]], dtype=torch.long)\n",
    "max_length = 20\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    output_logits = trained_model(input_sequence, translated_input, generate_mask=False)\n",
    "    output_probs = nn.functional.softmax(output_logits, dim=-1)\n",
    "    predicted_token_ids = torch.argmax(output_probs, dim=-1)\n",
    "    if predicted_token_ids[-1] == FRENCH_TOKEN_MAPPING[\"<EOS>\"] or len(predicted_token_ids) >= max_length:\n",
    "        done = True\n",
    "    translated_input = torch.cat([translated_input, predicted_token_ids[-1].unsqueeze(0)])\n",
    "    predicted_words = [list(FRENCH_TOKEN_MAPPING.keys())[list(FRENCH_TOKEN_MAPPING.values()).index(token_id)] for token_id in translated_input]\n",
    "    print(f\"Predicted Translation: {' '.join(predicted_words)}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
